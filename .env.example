# OpenStack Assistant Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM Provider Configuration (Choose ONE)
# =============================================================================

# Option 1: Google Gemini
# Get an API key at: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your-gemini-api-key-here

# Optional: Gemini model to use
# Options: gemini-2.5-flash, gemini-2.5-pro, gemini-2.0-flash, gemini-2.0-flash-lite
# Default: gemini-2.5-flash
GEMINI_MODEL=gemini-2.5-flash

# Option 2: Granite
# Requires both URL and User Key
# URL can be either:
#   - Full URL with model routing: https://granite-4-0-h-tiny--apicast-production.apps.example.com:443/v1/chat/completions
#   - Base URL: https://apicast-production.apps.example.com (will auto-append /v1/chat/completions)
# GRANITE_URL=https://your-granite-api-endpoint.com
# GRANITE_USER_KEY=your-granite-user-key-here

# Optional: Granite configuration
# GRANITE_TEMPERATURE=0.0
# GRANITE_MAX_TOKENS=4096
# GRANITE_CONTEXT_WINDOW=16384  # Maximum context window size in tokens (default: 8192)
#                                # For Granite 4.0 Tiny, try 16384 or 32768 if supported
# GRANITE_CONTEXT_TARGET_RATIO=0.75  # Target ratio of context to use (default: 0.6 = 60%)
#                                     # Increased to 0.75 to utilize more of available context
#                                     # Lower values = more aggressive trimming, more headroom
#                                     # Higher values = keep more history, less headroom

# Optional: MCP server URL (if using a remote MCP server)
# MCP_SERVER_URL=http://localhost:3000

# Optional: MCP server command (if using a local MCP server)
# This will be executed to start the server
# Example: npx @modelcontextprotocol/server-filesystem /tmp
# MCP_SERVER_COMMAND=npx @modelcontextprotocol/server-filesystem /etc/openstack

# Optional: Limit which MCP tools to load (comma-separated list)
# If not set, all tools from the MCP server will be available
# Example: MCP_ALLOWED_TOOLS=read_file,write_file,list_directory
# MCP_ALLOWED_TOOLS=

# Optional: Default workflow file to use
# WORKFLOW_FILE=/path/to/workflow.json

# Optional: System instruction file for chat mode
# This defines the agent's identity and behavior in interactive chat
# Example: examples/rhoso-upgrade-agent.txt
# SYSTEM_INSTRUCTION_FILE=/path/to/system-instruction.txt

# =============================================================================
# Debugging Configuration
# =============================================================================

# Optional: Directory to log raw LLM messages for debugging
# When set, all LLM requests and responses will be logged to JSON files
# Files are named sequentially: 0001_request.json, 0001_response.json, etc.
# This is useful for debugging MCP tool command processing issues
# Example: RAW_MESSAGE_LOG_DIR=./llm_logs
# RAW_MESSAGE_LOG_DIR=
